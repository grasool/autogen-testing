{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "\n",
    "# Load LLM inference endpoints from an env variable or a file\n",
    "# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n",
    "# and OAI_CONFIG_LIST_sample.json\n",
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n",
    "# This initiates an automated chat between the two agents to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import oai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "#client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "\n",
    "    # required but ignored\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "input_text = f\"Create a disjunctive normal form summary of the elagibilty, inclusion, and exclusion criteria using the following text: \\\n",
    "            ``` Bl bla bal ```    \"\n",
    "context = \"You are a helpful assistant who is able to answer questions about a variety of topics including \\\n",
    "    disjunctive normal form, medical temonologies, and clinical trials. You will be provided with the text \\\n",
    "         from clinical trials and you will be asked to create a disjunctive normal form summary of the elagibilty, \\\n",
    "            inclusion, and exclusion criteria. \"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"llama2\", # this field is currently unused\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": context},\n",
    "    {\"role\": \"user\", \"content\": input_text}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "response = completion.choices[0].message.content\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"llama2\",\n",
    "            \"api_key\": 'ollama',\n",
    "            \"base_url\": 'http://localhost:11434/v1/',\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"llama-7B\",\n",
    "            \"base_url\": \"http://localhost:11434/v1/\",\n",
    "            \"api_type\": \"openai\",\n",
    "            \"api_key\": 'ollama',\n",
    "        },\n",
    "    ],\n",
    "    \"temperature\": 0.9,\n",
    "    \"timeout\": 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautogen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AssistantAgent, UserProxyAgent, config_list_from_json\n\u001b[1;32m----> 2\u001b[0m assistant \u001b[38;5;241m=\u001b[39m \u001b[43mAssistantAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m UserProxyAgent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m, code_execution_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_docker\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}) \u001b[38;5;66;03m# IMPORTANT: set to True to run code in docker, recommended\u001b[39;00m\n\u001b[0;32m      4\u001b[0m user_proxy\u001b[38;5;241m.\u001b[39minitiate_chat(assistant, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlot a chart of NVDA and TESLA stock price change YTD.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\4475358\\.conda\\envs\\pyautogen\\lib\\site-packages\\autogen\\agentchat\\assistant_agent.py:61\u001b[0m, in \u001b[0;36mAssistantAgent.__init__\u001b[1;34m(self, name, system_message, llm_config, is_termination_msg, max_consecutive_auto_reply, human_input_mode, description, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     35\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     43\u001b[0m ):\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m        name (str): agent name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m            [ConversableAgent](conversable_agent#__init__).\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     62\u001b[0m         name,\n\u001b[0;32m     63\u001b[0m         system_message,\n\u001b[0;32m     64\u001b[0m         is_termination_msg,\n\u001b[0;32m     65\u001b[0m         max_consecutive_auto_reply,\n\u001b[0;32m     66\u001b[0m         human_input_mode,\n\u001b[0;32m     67\u001b[0m         llm_config\u001b[38;5;241m=\u001b[39mllm_config,\n\u001b[0;32m     68\u001b[0m         description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[0;32m     72\u001b[0m         log_new_agent(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlocals\u001b[39m())\n",
      "File \u001b[1;32mc:\\Users\\4475358\\.conda\\envs\\pyautogen\\lib\\site-packages\\autogen\\agentchat\\conversable_agent.py:154\u001b[0m, in \u001b[0;36mConversableAgent.__init__\u001b[1;34m(self, name, system_message, is_termination_msg, max_consecutive_auto_reply, human_input_mode, function_map, code_execution_config, llm_config, default_auto_reply, description)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    150\u001b[0m     ):\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either set llm_config to False, or specify a non-empty \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m either in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm_config\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or in each config of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_list\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         )\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m OpenAIWrapper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[0;32m    157\u001b[0m     log_new_agent(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlocals\u001b[39m())\n",
      "File \u001b[1;32mc:\\Users\\4475358\\.conda\\envs\\pyautogen\\lib\\site-packages\\autogen\\oai\\client.py:373\u001b[0m, in \u001b[0;36mOpenAIWrapper.__init__\u001b[1;34m(self, config_list, **base_config)\u001b[0m\n\u001b[0;32m    371\u001b[0m     config_list \u001b[38;5;241m=\u001b[39m [config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m config_list]  \u001b[38;5;66;03m# make a copy before modifying\u001b[39;00m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m config_list:\n\u001b[1;32m--> 373\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_default_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_config\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# could modify the config\u001b[39;00m\n\u001b[0;32m    374\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    375\u001b[0m             {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_kwargs}}\n\u001b[0;32m    376\u001b[0m         )\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\4475358\\.conda\\envs\\pyautogen\\lib\\site-packages\\autogen\\oai\\client.py:426\u001b[0m, in \u001b[0;36mOpenAIWrapper._register_default_client\u001b[1;34m(self, config, openai_config)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clients\u001b[38;5;241m.\u001b[39mappend(OpenAIClient(client))\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 426\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopenai_config)\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clients\u001b[38;5;241m.\u001b[39mappend(OpenAIClient(client))\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n",
      "File \u001b[1;32mc:\\Users\\4475358\\.conda\\envs\\pyautogen\\lib\\site-packages\\openai\\_client.py:98\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m     96\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m     )\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n",
    "# This initiates an automated chat between the two agents to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oai.ChatCompletion.create(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"model\": \"mistral\",\n",
    "            \"base_url\": \"http://localhost:11434\",\n",
    "            \"api_key\": \"NULL\",\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"llama-2\",\n",
    "            \"base_url\": \"http://localhost:11434\",\n",
    "            \"api_key\": \"NULL\",\n",
    "        }\n",
    "    ],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
